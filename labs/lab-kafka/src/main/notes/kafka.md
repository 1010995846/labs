# 介绍

> Apache Kafka is an open-source distributed event streaming platform.
> Apache Kafka 是一个开源的分布式流处理平台。

Kafka 的官网介绍提到的 3 种能力，都是基于这个描述，消息中间件只是其中一个用法。

- 数据的发布和订阅能力（消息队列）

- 数据的分布式存储能力（存储系统）

- 数据的实时处理能力（流处理引擎）

**流式处理**

Kafka 最开始其实是 Linkedin 内部孵化的项目，在设计之初是被当做「数据管道」，用于处理以下两种场景：

- 运营活动场景：记录用户的浏览、搜索、点击、活跃度等行为。

- 系统运维场景：监控服务器的 CPU、内存、请求耗时等性能指标。

可以看到这两种数据都属于日志范畴，特点是：数据实时生产，而且数据量很大。

所以从一开始，Kafka 就是为实时日志流而生的。了解了这个背景，就不难理解 Kafka 与流数据的关系了，以及 Kafka 为什么在大数据领域有如此广泛的应用？也是因为它最初就是为解决大数据的管道问题而诞生的。

**平台**

Kafka 从 0.8 版本开始，就已经在提供一些和数据处理有关的组件，并向「实时流处理平台」方向发展了，比如：

- Kafka Streams：一个轻量化的流计算库，性质类似于 Spark、Flink。

- Kafka Connect：一个数据同步工具，能将 Kafka 中的数据导入到关系数据库、Hadoop、搜索引擎中。

# MQ

## 消息模型

所谓消息模型，可以理解成一种逻辑结构，它是技术架构再往上的一层抽象，往往隐含了最核心的设计思想。

> 类似架构图、JVM内存模型等

**首先，为了将一份消息数据分发给多个消费者，并且每个消费者都能收到全量的消息，很自然的想到了广播，即多个消费者。**

![model](picture\model1.jpg)

**在此之上，消费者还需要筛选出自己想要的消息。**

因此就有了`Topic` 以及`发布-订阅模型`。生产者在发送消息时，通过`Topic Routing`这一层对消息进行逻辑上的分类，发送到对应的Topic队列。

> 为什么不routing后置？
>
> - 消息杂乱，分布在所有队列中；
> - 如果后置，routing的实现应该是轮询所有队列查找消息，并且还需要额外信息（如消息头）进行匹配，性能空间效率更低；
>
> routing前置的好处？
>
> - 消息分类，队列提纯；

![model2](picture\model2.jpg)

**解决多个消费者重复消费同一条消息。**

每个消费者消费自己的分区。

![model3](picture\model3.jpg)



消息进行了持久化存储，由消费者自己各取所需，想取哪个消息，想什么时候取都行，只需要传递一个消息的 offset 即可。

这样一个根本性改变，彻底将复杂的消费问题又转嫁给消费者了，这样使得 Kafka 本身的复杂度大大降低，从而为它的高性能和高扩展打下了良好的基础。（这是 Kafka 不同于 ActiveMQ 和 RabbitMQ 最核心的地方）

最后，简化一下，就是下面这张图：

![model-simple](picture\model-simple.jpg)

> 问题：
>
> 1. 分布式系统如何让消息仅被消费一次？
>
>    下文消费者的"集群消费能力"，即消费组

### 存储

面对海量数据，单机的存储容量和读写性能肯定有限，大家很容易想到一种存储方案：**对数据进行分片存储**。

> 这种方案在我们实际工作中也非常常见：
>
> - 数据库设计中，当单表的数据量达到几千万或者上亿时，我们会将它拆分成多个库或者多张表。
>
> - 缓存设计中，当单个 Redis 实例的数据量达到几十个 G 引发性能瓶颈时，我们会将单机架构改成分片集群架构。
>
> 类似的拆分思想在 HDFS、ElasticSearch 等中间件中都能看到。

Kafka 也不例外，它同样采用了这种水平拆分方案。在 Kafka 的术语中，拆分后的数据子集叫做 **Partition（分区）**，各个分区的数据合集即全量数据。

<img src="picture\model_partition.jpg" alt="model_partition" style="zoom: 80%;" />



同一个**Topic**下会分为多个**Partition**。主题路由将消息发往**Topic**分区，分区路由根据设定的分区规则发往**Partition**。

通过这样两层关系，最终在 **Topic** 之下，就有了一个新的划分单位：**Partition**。先通过 **Topic** 对消息进行逻辑分类，然后通过 **Partition** 进一步做物理分片，最终多个 **Partition** 又会均匀地分布在 *集群* 中的每台机器上，从而很好地解决了存储的扩展性问题。

> 问题：
>
> 1、那么消费者如何从Partition拉取消息？
>
> 2、Topic在同一台机器上可有多个Partition分区吗？

因此，**Partition** 是 Kafka 最基本的部署单元。

- Partition 是存储的关键所在，MQ「一发一存一消费」的核心流程必然围绕它展开。

- Kafka 高并发设计中最难的三高问题都能和 Partition 关联起来。

以 **Partition** 作为根，能很自然地联想出 Kafka 架构设计中的各个知识点，形成可靠的知识体系。

# 架构设计

假设现在有两个 Topic，每个 Topic 都设置了两个 Partition，如果 Kafka 集群是两台机器，部署架构将会是下面这样：

<img src="picture\arch.jpg" alt="service" style="zoom:80%;" />

可以看到：同一个 Topic 的两个 Partition 分布在不同的消息服务器上，能做到消息的分布式存储了。但是对于 Kafka 这个高并发系统来说，仅存储可扩展还不够，消息的拉取也必须并行才行，否则会遇到极大的性能瓶颈。

那我们再看看消费端，它又是如何跟 Partition 结合并做到并行处理的？

从消费者来看，首先要满足两个基本诉求：

- **广播消费能力**：同一个 Topic 可以被多个消费者订阅，一条消息能够被消费多次。

- **集群消费能力**：当消费者本身也是集群时，每一条消息只能分发给集群中的一个消费者进行处理。

为了满足这两点要求，Kafka 引出了**消费组**的概念，每个消费者都有一个对应的消费组，**组间进行广播消费，组内进行集群消费**。此外，Kafka 还限定了：**每个 Partition 可被多个消费组消费（广播），但只能由消费组中的一个消费者进行消费（集群）**。

最终的消费关系如下图所示：假设主题 A 共有 4 个分区，消费组 2 只有两个消费者，最终这两个消费组将平分整个负载，各自消费两个分区的消息。

<img src="picture\consumer_group.jpg" alt="consumer_group" style="zoom: 80%;" />

## 可用性

消费组解决了集群消费问题，消费者集群可用性由集群自行解决。

消息服务器集群可解决某台服务宕机导致的消息服务无法访问，但存储在宕机服务的Partition消息不就无法访问。对消息进行持久化存储。但是持久化只能解决一部分问题，它只能确保机器重启后，历史数据不丢失。但在机器恢复之前，这部分数据将一直无法访问。

Partition 的多副本机制。在 Kafka 集群中，每个 Partition 都有多个副本，同一分区的不同副本中保存的是相同的消息。

副本之间是 “一主多从” 的关系，其中 leader 副本负责读写请求，follower 副本只负责和 leader 副本同步消息，当 leader 副本发生故障时，它才有机会被选举成新的 leader 副本并对外提供服务，否则一直是待命状态。

现在，我假设 Kafka 集群中有 4 台服务器，主题 A 和主题 B 都有两个 Partition，且每个 Partition 各有两个副本，那最终的多副本架构将如下图所示：

<img src="picture\cluster_follower.jpg" alt="cluster_follower" style="zoom: 80%;" />

即大多数组件具有的故障转移能力。

Kafka 最终整体架构图：

<img src="picture\arch_final.jpg" alt="arch_final" style="zoom:67%;" />

1、Producer：生产者，负责创建消息，然后投递到 Kafka 集群中，投递时需要指定消息所属的 Topic，同时确定好发往哪个 Partition。

> routing呢？Kafka不负责这个？

2、Consumer：消费者，会根据它所订阅的 Topic 以及所属的消费组，决定从哪些 Partition 中拉取消息。

3、Broker：消息服务器，可水平扩展，负责分区管理、消息的持久化、故障自动转移等。

4、Zookeeper：负责集群的元数据管理等功能，比如集群中有哪些 broker 节点以及 Topic，每个 Topic 又有哪些 Partition 等。







1、Kafka 通过巧妙的模型设计，将自己退化成一个海量消息的存储系统。

2、为了解决存储的扩展性问题，Kafka 对数据进行了水平拆分，引出了 Partition（分区），这是 Kafka 部署的基本单元，同时也是 Kafka 并发处理的最小粒度。

3、对于一个高并发系统来说，还需要做到高可用，Kafka 通过 Partition 的多副本冗余机制进行故障转移，确保了高可靠。

# 存储设计

为什么 Kafka 会采用 Logging（日志文件）的存储方式？它的选型依据到底是什么？

## 需求

我们试着从以下两个维度来分析下：

> 1、功能性需求：存的是什么数据？量级如何？需要存多久？CRUD 的场景都有哪些？
>
> 2、非功能性需求：性能和稳定性的要求是什么样的？是否要考虑扩展性？

**1、存的数据主要是消息流**：消息可以是最简单的文本字符串，也可以是自定义的复杂格式。

但是对于 Broker 来说，它只需处理好消息的投递即可，无需关注消息内容本身。

**2、数据量级非常大**：因为 Kafka 作为 Linkedin 的孵化项目诞生，用作实时日志流处理（运营活动中的埋点、运维监控指标等），按 Linkedin 当初的业务规模来看，每天要处理的消息量预计在千亿级规模。

**3、CRUD 场景足够简单：**因为消息队列最核心的功能就是数据管道，它仅提供转储能力，因此 CRUD 操作确实很简单。

首先，消息等同于通知事件，都是追加写入的，根本无需考虑 update。其次，对于 Consumer 端来说，Broker 提供按 offset（消费位移）或者 timestamp（时间戳）查询消息的能力就行。再次，长时间未消费的消息（比如 7 天前的），Broker 做好定期删除即可。

首先，消息等同于通知事件，都是追加写入的，根本无需考虑 update。其次，对于 Consumer 端来说，Broker 提供按 offset（消费位移）或者 timestamp（时间戳）查询消息的能力就行。再次，长时间未消费的消息（比如 7 天前的），Broker 做好定期删除即可。

接着，我们再来看看非功能性需求：

**1、性能要求：**之前的文章交代过，Linkedin 最初尝试过用 ActiveMQ 来解决数据传输问题，但是性能无法满足要求，然后才决定自研 Kafka。ActiveMQ 的单机吞吐量大约是万级 TPS，Kafka 显然要比 ActiveMQ 的性能高一个量级才行。

**2、稳定性要求：**消息的持久化（确保机器重启后历史数据不丢失）、单台 Broker 宕机后如何快速故障转移继续对外提供服务，这两个能力也是 Kafka 必须要考虑的。

**3、扩展性要求：**Kafka 面对的是海量数据的存储问题，必然要考虑存储的扩展性。

再简单总结下，Kafka 的存储需求如下：

> 1、功能性需求：其实足够简单，追加写、无需update、能根据消费位移和时间戳查询消息、能定期删除过期的消息。
>
> 2、非功能性需求：是难点所在，因为 Kafka 本身就是一个高并发系统，必然会遇到典型的高性能、高可用和高扩展这三方面的挑战。

## 选型

根据需求，Kafka 所处业务场景的特点是：

> 1、写入操作：并发非常高，百万级 TPS，但都是顺序写入，无需考虑更新
>
> 2、查询操作：需求简单，能按照 offset 或者 timestamp 查询消息即可

如果单纯满足 Kafka 百万级 TPS 的写入操作需求，采用 Append 追加写日志文件的方式显然是最理想的，前面讲过磁盘顺序写的性能完全是可以满足要求的。

剩下的就是如何解决高效查询的问题。如果采用 B Tree 类的索引结构来实现，每次数据写入时都需要维护索引（属于随机 IO 操作），而且还会引来“页分裂”等比较耗时的操作。而这些代价对于仅需要实现简单查询要求的 Kafka 来说，显得非常重。所以，B Tree 类的索引并不适用于 Kafka。

相反，哈希索引看起来却非常合适。为了加快读操作，如果只需要在内存中维护一个「从 offset 到日志文件偏移量」的映射关系即可，每次根据 offset 查找消息时，从哈希表中得到偏移量，再去读文件即可。（根据 timestamp 查消息也可以采用同样的思路）

但是哈希索引常驻内存，显然没法处理数据量很大的情况，Kafka 每秒可能会有高达几百万的消息写入，一定会将内存撑爆。

可我们发现消息的 offset 完全可以设计成有序的（实际上是一个单调递增 long 类型的字段），这样消息在日志文件中本身就是有序存放的了，我们便没必要为每个消息建 hash 索引了，完全可以将消息划分成若干个 block，只索引每个 block 第一条消息的 offset 即可，先根据大小关系找到 block，然后在 block 中顺序搜索，这便是 Kafka “稀疏索引” 的来源。

最终我们发现：Append 追加写日志 + 稀疏的哈希索引，形成了 Kafka 最终的存储方案。

## 结构

可以看到，Kafka 是一个「分区 + 分段 + 索引」的三层结构：

<img src="picture\memory_structure.jpg" alt="memory_structure" style="zoom:67%;" />

1、每个 Topic 被分成多个 Partition，Partition 从物理上可以理解成一个文件夹。

之前的文章解释过：Partition 主要是为了解决 Kafka 存储上的水平扩展问题，如果一个 Topic 的所有消息都只存在一个 Broker，这个 Broker 必然会成为瓶颈。因此，将 Topic 内的数据分成多个 Partition，然后分布到整个集群是很自然的设计方式。

2、每个 Partition 又被分成了多个 Segment，Segment 从物理上可以理解成一个「数据文件 + 索引文件」，这两者是一一对应的。



一定有读者会有疑问：有了 Partition 之后，为什么还需要 Segment？

如果不引入 Segment，一个 Partition 只对应一个文件，那这个文件会一直增大，势必造成单个 Partition 文件过大，查找和维护不方便。

此外，在做历史消息删除时，必然需要将文件前面的内容删除，不符合 Kafka 顺序写的思路。而在引入 Segment 后，则只需将旧的 Segment 文件删除即可，保证了每个 Segment 的顺序写。

# 高性能设计

这其实是一个系统性的问题，至少需要深入到操作系统层面，从 CPU 和存储入手，去了解底层的实现机制，然后再自底往上，一层一层去解密和贯穿起来。

但是站在更高的视角来看，我认为：高性能设计其实万变不离其宗，一定是从**「计算和 IO」**这两个维度出发，去考虑可能的优化点。

**那「计算」维度的性能优化手段有哪些呢？无外乎这两种方式：**

> 1、让更多的核来参与计算：比如用多线程代替单线程、用集群代替单机等。
>
> 2、减少计算量：比如用索引来取代全局扫描、用同步代替异步、通过限流来减少请求处理量、采用更高效的数据结构和算法等。

**再看下「IO」维度的性能优化手段又有哪些?** 可以通过 Linux 系统的 IO 栈图来辅助思考。

<img src="picture\linux_IO_stack.png" alt="linux_IO_stack" style="zoom:50%;" />

可以看到，整个 IO 体系结构是分层的，我们能够从应用程序、操作系统、磁盘等各个层次来考虑性能优化，而所有这些手段又几乎围绕以下两个方面展开：

> 1、加快 IO 速度：比如用磁盘顺序写代替随机写、用 NIO 代替 BIO、用性能更好的 SSD 代替机械硬盘等。
>
> 2、减少 IO 次数或者 IO 数据量：比如借助系统缓存或者外部缓存、通过零拷贝技术减少 IO 复制次数、批量读写、数据压缩等。

上面这些内容可以理解成高性能设计的部分方法。

不管 Kafka 、RocketMQ 还是其他消息队列，其本质都是「一发一存一消费」。

基于这个思路，便形成了下面这张 Kafka 高性能设计的全景图，我按照生产消息、存储消息、消费消息 3 个模块，将 Kafka 最具代表性的 12 条性能优化手段做了归类。

<img src="picture\function.jpg" alt="function" style="zoom:67%;" />

## 生产

1. **批量发送消息**

Kafka 作为一个消息队列，很显然是一个 IO 密集型应用，它所面临的挑战除了磁盘 IO（Broker 端需要对消息持久化），还有网络 IO（Producer 到 Broker，Broker 到 Consumer，都需要通过网络进行消息传输）。

在上一篇文章已经指出过：磁盘顺序 IO 的速度其实非常快，不亚于内存随机读写。这样网络 IO 便成为了 Kafka 的性能瓶颈所在。

基于这个背景， Kafka 采用了批量发送消息的方式，通过将多条消息按照分区进行分组，然后每次发送一个消息集合，从而大大减少了网络传输的 overhead。

看似很平常的一个手段，其实它大大提升了 Kafka 的吞吐量，而且它的精妙之处远非如此，下面几条优化手段都和它息息相关。

> 批量发送不是生产者的设计吗？和Kafka有什么关系

2. **消息压缩**

消息压缩的目的是为了进一步减少网络传输带宽。而对于压缩算法来说，通常是：**数据量越大，压缩效果才会越好。**

因为有了批量发送这个前期，从而使得 Kafka 的消息压缩机制能真正发挥出它的威力（压缩的本质取决于多消息的重复性）。对比压缩单条消息，同时对多条消息进行压缩，能大幅减少数据量，从而更大程度提高网络传输率。

其实压缩消息不仅仅减少了网络 IO，它还大大降低了磁盘 IO。因为批量消息在持久化到 Broker 中的磁盘时，仍然保持的是压缩状态，最终是在 Consumer 端做了解压缩操作。

这种端到端的压缩设计，其实非常巧妙，它又大大提高了写磁盘的效率。

> 同上

3. **高效序列化**

Kafka 消息中的 Key 和 Value，都**支持自定义的序列化和反序列化器**。

因此，用户可以根据实际情况选用快速且紧凑的序列化方式（比如 ProtoBuf、Avro）来减少实际的网络传输量以及磁盘存储量，进一步提高吞吐量。

4. **内存池复用**

前面说过 Producer 发送消息是批量的，因此消息都会先写入 Producer 的内存中进行缓冲，直到多条消息组成了一个 Batch，才会通过网络把 Batch 发给 Broker。

当这个 Batch 发送完毕后，显然这部分数据还会在 Producer 端的 JVM 内存中，由于不存在引用了，它是可以被 JVM 回收掉的。

但是大家都知道，JVM GC 时一定会存在 Stop The World 的过程，即使采用最先进的垃圾回收器，也势必会导致工作线程的短暂停顿，这对于 Kafka 这种高并发场景肯定会带来性能上的影响。

有了这个背景，便引出了 Kafka 非常优秀的内存池机制，它和连接池、线程池的本质一样，都是为了提高复用，减少频繁的创建和释放。

具体是如何实现的呢？其实很简单：Producer 一上来就会占用一个固定大小的内存块，比如 64MB，然后将 64 MB 划分成 M 个小内存块（比如一个小内存块大小是 16KB）。

当需要创建一个新的 Batch 时，直接从内存池中取出一个 16 KB 的内存块即可，然后往里面不断写入消息，但最大写入量就是 16 KB，接着将 Batch 发送给 Broker ，此时该内存块就可以还回到缓冲池中继续复用了，根本不涉及垃圾回收。最终整个流程如下图所示：

<img src="picture\producer_mermory.jpg" alt="producer_mermory" style="zoom:67%;" />

了解了 Producer 端上面 4 条高性能设计后，大家一定会有一个疑问：传统的数据库或者消息中间件都是想办法让 Client 端更轻量，将 Server 设计成重量级，仅让 Client 充当应用程序和 Server 之间的接口。

但是 Kafka 却反其道而行之，采取了独具一格的设计思路，在将消息发送给 Broker 之前，需要先在 Client 端完成大量的工作，例如：消息的分区路由、校验和的计算、压缩消息等。这样便很好地分摊 Broker 的计算压力。

可见，没有最好的设计，只有最合适的设计，这就是架构的本源。

## 存储

1. **IO 多路复用**

对于 Kafka Broker 来说，要做到高性能，首先要考虑的是：设计出一个高效的网络通信模型，用来处理它和 Producer 以及 Consumer 之间的消息传递问题。

很典型的 Reactor 网络通信模型

![network_model_reactor](picture\network_model_reactor.jpg)

通俗点记忆就是 1 + N + M：

> 1：表示 1 个 Acceptor 线程，负责**监听**新的连接，然后将新连接交给 Processor 线程处理。
>
> N：表示 N 个 Processor 线程，每个 Processor 都有自己的 selector，负责从 socket 中**读写**数据，然后**投递**到Queue中。
>
> M：表示 M 个 KafkaRequestHandler 业务处理线程，它通过调用 KafkaApis 进行业务处理，然后生成 response，再交由给 Processor 线程。

> 回头看看xxl-job笔记

Reactor 模式正是采用了很经典的 IO 多路复用技术，它可以复用一个线程去处理大量的 Socket 连接，从而保证高性能。Netty 和 Redis 为什么能做到十万甚至百万并发？它们其实都采用了 Reactor 网络通信模型。

2. **磁盘顺序写**

Kafka 作为消息队列，本质上就是一个队列，是先进先出的，而且消息一旦生产了就不可变。

这种有序性和不可变性使得 Kafka 完全可以「顺序写」日志文件，也就是说，仅仅将消息追加到文件末尾即可。

> 具体分析见上文《存储设计》

3. **Page Cache**

这里 Kafka 用到了 Page Cache 技术，简单理解就是：利用了操作系统本身的缓存技术，在读写磁盘日志文件时，其实操作的都是内存，然后由操作系统决定什么时候将 Page Cache 里的数据真正刷入磁盘。

> 也是很常见的操作。数据读写时，先利用内存的高效，暂存在内存中，达到定量或定时后再批量交互，降低IO，交互目标不一定是磁盘。

<img src="picture\page_cache.jpg" alt="page_cache" style="zoom: 67%;" />

Page Cache 缓存的是最近会被使用的磁盘数据，利用的是「时间局部性」原理，依据是：最近访问的数据很可能接下来再访问到。而预读到 Page Cache 中的磁盘数据，又利用了「空间局部性」原理，依据是：数据往往是连续访问的。

而 Kafka 作为消息队列，消息先是顺序写入，而且立马又会被消费者读取到，无疑非常契合上述两条局部性原理。因此，页缓存可以说是 Kafka 做到高吞吐的重要因素之一。

除此之外，页缓存还有一个巨大的优势。用过 Java 的人都知道：如果不用页缓存，而是用 JVM 进程中的缓存，对象的内存开销非常大（通常是真实数据大小的几倍甚至更多），此外还需要进行垃圾回收，GC 所带来的 Stop The World 问题也会带来性能问题。可见，页缓存确实优势明显，而且极大地简化了 Kafka 的代码实现。 

4. **分区分段结构**

上文《消息模型》中分析了Kafka的存储结构。

在分区**Partition**之下还有一层物理结构：那便是「分段」**Segment**。

简单理解：分区对应的其实是文件夹（逻辑分区），分段对应的才是真正的日志文件（物理分段）。

<img src="picture\log_segment.jpg" alt="log_segment" style="zoom:67%;" />

如果不引入 Segment，一个 Partition 只对应一个文件，那这个文件会一直增大，势必造成单个 Partition 文件过大，查找和维护不方便。

此外，在做历史消息删除时，必然需要将文件前面的内容删除，只有一个文件显然不符合 Kafka 顺序写的思路。而在引入 Segment 后，则只需将旧的 Segment 文件删除即可，保证了每个 Segment 的顺序写。

## 消费

1. **稀疏索引**

Kafka 所面临的查询场景其实很简单：能按照 offset 或者 timestamp 查到消息即可。

> 上文《存储设计-需求》分析过

为了加快读操作，如果只需要在内存中维护一个「从 offset 到日志文件偏移量」的映射关系即可，每次根据 offset 查找消息时，从哈希表中得到偏移量，再去读文件即可。（根据 timestamp 查消息也可以采用同样的思路）

但是哈希索引常驻内存，显然没法处理数据量很大的情况，Kafka 每秒可能会有高达几百万的消息写入，一定会将内存撑爆。

可我们发现消息的 offset 完全可以设计成有序的（实际上是一个单调递增 long 类型的字段），这样消息在日志文件中本身就是有序存放的了，我们便没必要为每个消息建 hash 索引了，完全可以将消息划分成若干个 block，只索引每个 block 第一条消息的 offset 即可，先根据大小关系找到 block，然后在 block 中顺序搜索，这便是 Kafka “稀疏索引” 的设计思想。

![hash_index](picture\hash_index.png)

采用 “稀疏索引”，可以认为是在磁盘空间、内存空间、查找性能等多方面的一个折中。有了稀疏索引，当给定一个 offset 时，Kafka 采用的是二分查找来高效定位不大于 offset 的物理位移，然后找到目标消息。

2. **mmap**

利用稀疏索引，已经基本解决了高效查询的问题，但是这个过程中仍然有进一步的优化空间，那便是通过 **mmap（memory mapped files）** 读写上面提到的稀疏索引文件，进一步提高查询消息的速度。

> 注意：mmap 和 page cache 是两个概念，网上很多资料把它们混淆在一起。
>
> 此外，还有资料谈到 Kafka 在读 log 文件时也用到了 mmap，通过对 2.8.0 版本的源码分析，这个信息也是错误的，其实只有索引文件的读写才用到了 mmap.

究竟如何理解 mmap？前面提到，常规的文件操作为了提高读写性能，使用了 Page Cache 机制，但是由于页缓存处在内核空间中，不能被用户进程直接寻址，所以读文件时还需要通过系统调用，将页缓存中的数据再次拷贝到用户空间中。

而采用 mmap 后，它将磁盘文件与进程虚拟地址做了映射，并不会招致系统调用，以及额外的内存 copy 开销，从而提高了文件读取效率。

> Page Cache是缓存机制；
>
> mmap根据日志文件的稀疏索引，进程直接读取文件。

3. **零拷贝**

消息借助稀疏索引被查询到后，下一步便是：将消息从磁盘文件中读出来。

Kafka 用到了零拷贝（Zero-Copy）技术来提升性能。所谓的零拷贝是指数据直接从磁盘文件复制到网卡设备，而无需经过应用程序，减少了内核和用户模式之间的上下文切换。

![file_copy](picture\file_copy.jpg)

如果采用零拷贝技术（底层通过 sendfile 方法实现），流程将变成下面这样。可以看到：只需 3 次拷贝以及 2 次上下文切换，显然性能更高。

![file_copy_zero](picture\file_copy_zero.jpg)

4. **批量拉取和压缩**

和生产者批量发送消息类似，消息者也是批量拉取压缩消息的，每次拉取一个消息集合，从而大大减少了网络传输的 overhead。